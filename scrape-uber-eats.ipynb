{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python39564bitpythonprojectsal79l2lnvenv3053deee5581460eb6d4cb6a5511928a",
   "display_name": "Python 3.9.5 64-bit ('python_projects-aL79L2lN': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import slugify\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"phoenix_restaurant_urls.txt\") as f:\n",
    "    urls = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [url for url in urls if len(url.split(\"/\")) == 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_stores = ['walgreens', 'airshop', 'convenient-food-mart', 'liquor', 'convenience', 'food-mart', 'mart', 'groceries', 'rite-aid', 'mercado']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [url for url in urls if not any([store in url for store in excluded_stores])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_uber_eats_menu(url, recursion_depth = 0, menu_number = 1):\n",
    "\n",
    "    driver.get(url)\n",
    "    sleep(1)\n",
    "\n",
    "    try:\n",
    "        popup = driver.find_element_by_xpath(\"/html/body/div[1]/div/div[4]/div/div/div[2]/div[2]/button\")\n",
    "        popup.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        name = driver.find_element_by_xpath(\n",
    "            \"/html/body/div[1]/div/main/div[2]/div/div[3]/div[3]/div[1]/div[2]/div[2]/h1\"\n",
    "            ).text\n",
    "        print(f\"Looking up {name}...\")\n",
    "    except:\n",
    "        print(f\"No restaurant found at {url}\")\n",
    "        return\n",
    "\n",
    "    city_state = driver.find_element_by_xpath(\"/html/body/div[1]/div/main\").text\n",
    "    match = re.search(\",\\s(\\w+),\\s([A-Z][A-Z])\\s\\d\\d\\d\\d\\d\", city_state)\n",
    "    city = match[1]\n",
    "    state = match[2]\n",
    "        \n",
    "    menu_xpath = f\"/html/body/div[1]/div/main/div[4]/ul[{menu_number}]\"\n",
    "    try:\n",
    "        menu = driver.find_element_by_xpath(menu_xpath)\n",
    "    except NoSuchElementException:\n",
    "        print(f\"No menu found at {url}\")\n",
    "        return\n",
    "\n",
    "    # Etra menus\n",
    "    if recursion_depth == 0:\n",
    "        try:\n",
    "            extra_menus = driver.find_element_by_xpath(\"/html/body/div[1]/div/main/div[4]/div[1]\")\n",
    "            extra_menu_links = extra_menus.find_elements_by_tag_name(\"a\")[1:]\n",
    "            extra_menu_urls = [link.get_property(\"href\") for link in extra_menu_links]\n",
    "        except NoSuchElementException:\n",
    "            extra_menu_urls = []\n",
    "            print(\"Only one menu found\")\n",
    "            pass\n",
    "\n",
    "    item_location = driver.find_element_by_xpath(menu_xpath + \"/li[1]/ul/li[1]/div/div/div/div[1]\")\n",
    "\n",
    "    menu_items = menu.find_elements_by_class_name(item_location.get_attribute(\"class\").replace(\" \", \".\"))\n",
    "\n",
    "    item_name_class = driver.find_element_by_xpath(\n",
    "        menu_xpath + \"/li[1]/ul/li[1]/div/div/div/div[1]/div[1]/h4/div\"\n",
    "        ).get_attribute(\"class\").replace(\" \", \".\")\n",
    "\n",
    "    try:\n",
    "        item_price_class = driver.find_element_by_xpath(\n",
    "            menu_xpath + \"/li[1]/ul/li[1]/div/div/div/div[1]/div[3]/div\"\n",
    "            ).get_attribute(\"class\").replace(\" \", \".\")\n",
    "    except NoSuchElementException:\n",
    "        try: \n",
    "            item_price_class = driver.find_element_by_xpath(\n",
    "                menu_xpath + \"/li[1]/ul/li[1]/div/div/div/div[1]/div[2]/div\"\n",
    "                ).get_attribute(\"class\").replace(\" \", \".\")\n",
    "        except NoSuchElementException:\n",
    "            print(\"No prices found.\")\n",
    "            return\n",
    "\n",
    "    try:\n",
    "        item_cals_class = driver.find_element_by_xpath(\n",
    "            menu_xpath + \"/li[1]/ul/li/div/div/div/div[1]/div[2]/div[2]\"\n",
    "            ).get_attribute(\"class\").replace(\" \", \".\")\n",
    "    except NoSuchElementException:\n",
    "        item_cals_class = None\n",
    "\n",
    "    items_dict = {'name': [], 'price_usd': [], 'calories': []}\n",
    "\n",
    "    for item in menu_items:  \n",
    "\n",
    "        calories = ''\n",
    "\n",
    "        # Get item name  \n",
    "        try: \n",
    "            item_name = item.find_element_by_class_name(item_name_class).text\n",
    "            item_name = item_name.upper().strip()\n",
    "        except NoSuchElementException:\n",
    "            continue\n",
    "        \n",
    "        if \" CAL \" in item_name:\n",
    "            item_name, calories_raw = item_name.split(\" CAL \")\n",
    "            try:\n",
    "                calories_max = re.findall(\"\\d+\", calories_raw)[-1]\n",
    "                calories = float(calories_max)\n",
    "            except: \n",
    "                pass\n",
    "\n",
    "        # Price\n",
    "        try:\n",
    "            item_price_text = item.find_element_by_class_name(item_price_class).text\n",
    "            item_price_text_cleaned = item_price_text.lower().strip()\n",
    "            if item_price_text_cleaned in [\"customize\", \"unavailable\", \"\"]:\n",
    "                continue\n",
    "            item_price = float(item_price_text_cleaned.replace(\"$\", \"\"))\n",
    "        except (NoSuchElementException, ValueError):\n",
    "            continue\n",
    "        \n",
    "        # Calories\n",
    "        if item_cals_class is not None and calories == '':\n",
    "            try:\n",
    "                calories_raw = item.find_elements_by_class_name(item_cals_class)[1].text\n",
    "                calories_max = re.findall(\"\\d+\", calories_raw)[-1]\n",
    "                calories = float(calories_max)\n",
    "            except (NoSuchElementException, IndexError):\n",
    "                pass\n",
    "\n",
    "        items_dict['name'].append(item_name)\n",
    "        items_dict['price_usd'].append(item_price)\n",
    "        items_dict['calories'].append(calories)\n",
    "\n",
    "    df = pd.DataFrame(items_dict)\n",
    "\n",
    "    df['restaurant_name'] = name.upper().strip()\n",
    "    df['identifier'] = f'UBEREATS, {city.upper()}, {state}'\n",
    "    df['sugars_g'] = ''\n",
    "    df['cholesterol_mg'] = ''\n",
    "    df['protein_g'] = ''\n",
    "    df['fiber_g'] = ''\n",
    "    df['fat_g'] = ''\n",
    "    df['carbohydrates_g'] = ''\n",
    "    df['sodium_mg'] = ''\n",
    "\n",
    "    if len(df) == 0:\n",
    "        print(\"No prices or caloric information found.\")\n",
    "        return\n",
    "\n",
    "    for char in invalid_chars:\n",
    "        df['name'] = df['name'].str.replace(char, \"\")\n",
    "        df['restaurant_name'] = df['restaurant_name'].str.replace(char, \"\")\n",
    "\n",
    "    branch_name = f\"UberEATS-{city.title()}-{name.title()}\"\n",
    "    slug = slugify.slugify(branch_name)\n",
    "\n",
    "    print(\"=\"*10 + \"Data sample\" + \"=\"*10)\n",
    "    print(df.loc[:,\"name\":\"identifier\"].head(3).to_markdown())\n",
    "    print(\"=\"*30)\n",
    "\n",
    "    if recursion_depth == 0:\n",
    "        for extra_menu_number, new_url in enumerate(extra_menu_urls):\n",
    "            df_deep = scrape_uber_eats_menu(new_url, recursion_depth = 1, menu_number = extra_menu_number + 2)\n",
    "            df = pd.concat([df, df_deep])\n",
    "        \n",
    "        df = df.drop_duplicates('name')\n",
    "        df.to_csv(f\"./all_cities/{slug}.csv\", index = False)\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "driver = webdriver.Firefox()\n",
    "driver.maximize_window()\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "invalid_chars = [\":registered:\", \":tm:\", \":copyright:\",\"â„ \", \"*\", '\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/342 [00:00<?, ?it/s]Looking up Romeros Mexican Food...\n",
      "  0%|          | 1/342 [00:09<55:54,  9.84s/it]==========Data sample==========\n",
      "|    | name                                   |   price_usd | calories   | restaurant_name      | identifier            |\n",
      "|---:|:---------------------------------------|------------:|:-----------|:---------------------|:----------------------|\n",
      "|  0 | CHILE RELLENO PLATE AND RICE AND BEANS |           7 |            | ROMEROS MEXICAN FOOD | UBEREATS, PHOENIX, AZ |\n",
      "|  1 | GREEN CHILE BURRITO                    |           5 |            | ROMEROS MEXICAN FOOD | UBEREATS, PHOENIX, AZ |\n",
      "|  2 | CARNITAS PLATE AND RICE AND BEANS      |           8 |            | ROMEROS MEXICAN FOOD | UBEREATS, PHOENIX, AZ |\n",
      "==============================\n",
      "Looking up Long Wong's...\n",
      "  1%|          | 2/342 [00:16<44:47,  7.90s/it]==========Data sample==========\n",
      "|    | name              |   price_usd | calories   | restaurant_name   | identifier            |\n",
      "|---:|:------------------|------------:|:-----------|:------------------|:----------------------|\n",
      "|  0 | VALUE MEAL        |        9.29 |            | LONG WONG'S       | UBEREATS, PHOENIX, AZ |\n",
      "|  1 | WINGS             |       21.7  |            | LONG WONG'S       | UBEREATS, PHOENIX, AZ |\n",
      "|  2 | SHRIMP FRIED RICE |        8.25 |            | LONG WONG'S       | UBEREATS, PHOENIX, AZ |\n",
      "==============================\n",
      "Looking up Filibertos Mexican Food (Baseline & 32nd St)...\n",
      "  1%|          | 2/342 [00:22<1:04:03, 11.31s/it]==========Data sample==========\n",
      "|    | name            |   price_usd | calories   | restaurant_name                              | identifier            |\n",
      "|---:|:----------------|------------:|:-----------|:---------------------------------------------|:----------------------|\n",
      "|  0 | COUNTRY BURRITO |        7.2  |            | FILIBERTOS MEXICAN FOOD (BASELINE & 32ND ST) | UBEREATS, PHOENIX, AZ |\n",
      "|  1 | BACON BURRITO   |        7.53 |            | FILIBERTOS MEXICAN FOOD (BASELINE & 32ND ST) | UBEREATS, PHOENIX, AZ |\n",
      "|  2 | CHORIZO BURRITO |        7.4  |            | FILIBERTOS MEXICAN FOOD (BASELINE & 32ND ST) | UBEREATS, PHOENIX, AZ |\n",
      "==============================\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "scrape_uber_eats_menu() got multiple values for argument 'recursion_depth'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-32356cbeab5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m770\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mscrape_uber_eats_menu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-62-113f8ccd0cac>\u001b[0m in \u001b[0;36mscrape_uber_eats_menu\u001b[0;34m(url, recursion_depth, menu_number)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrecursion_depth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mextra_menu_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_url\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_menu_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mdf_deep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_uber_eats_menu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmenu_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextra_menu_number\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_deep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: scrape_uber_eats_menu() got multiple values for argument 'recursion_depth'"
     ]
    }
   ],
   "source": [
    "for url in tqdm(urls[770:]):\n",
    "    scrape_uber_eats_menu(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}